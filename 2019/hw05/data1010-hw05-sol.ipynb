{"nbformat":4,"nbformat_minor":0,"metadata":{"jupytext":{"formats":"ipynb,md"},"kernelspec":{"name":"julia-1.2","language":"julia","display_name":"Julia 1.2.0"},"language_info":{"file_extension":".jl","name":"julia","mimetype":"application/julia","version":"1.2.0"},"kernel_info":{"name":"julia-1.2"},"nteract":{"version":"0.15.0"},"colab":{"name":"data1010-hw05-sol.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZYQrlCMp_Fhi","colab_type":"text"},"source":["<style>\n","@media print\n","{\n","h2 {page-break-before:always}\n","}\n","</style>\n","\n","# Homework 05\n","\n","### Brown University  \n","### DATA 1010  \n","### Fall 2019"]},{"cell_type":"markdown","metadata":{"id":"ug_J09yV_Fhl","colab_type":"text"},"source":["## Problem 1\n","\n","Label each of the following four estimators as either (i) biased and\n","  consistent, (ii) biased and inconsistent, (iii) unbiased and\n","  consistent, or (iv) unbiased and inconsistent.  The matching will be\n","  one-to-one.\n","\n","(a) $X_1, X_2, \\ldots$ are i.i.d. Bernoulli random variables with\n","    unknown $p$ and estimator \n","    $$\\widehat{p} = \\frac{1}{n}\\sum^n_{i=1}X_i$$\n","\n","(b) $X_1, X_2, \\ldots$ are i.i.d. $\\mathcal{N}(\\mu,\\sigma^2)$, with\n","    unknown $\\mu$ and $\\sigma^2$ and estimator \n","    $$\\widehat{\\sigma}^2 = \\frac{\\displaystyle{\\sum^n_{i=1}(X_i-\\bar{X})^2}}{n}$$\n","\n","(c) $X_1, X_2, \\ldots$ are i.i.d. uniform random variables on an\n","    unknown bounded interval. For $n\\geq 100$ we estimate the\n","    mean using\n","    $$\\widehat{\\mu} = \\frac{\\displaystyle{\\sum^{100}_{i=1}X_i}}{100}$$\n","    \n","(d) $X_1, X_2, \\ldots$ are i.i.d. $\\mathcal{N}(\\mu,\\sigma^2)$, with\n","    unknown $\\mu$ and $\\sigma^2$. For $n\\geq 100$ we estimate the\n","    standard deviation using\n","    $$\\widehat{\\sigma} = \\sqrt{\\frac{\\displaystyle{\\sum^{100}_{i=1}(X_i-\\overline{X})^2}}{99}}$$"]},{"cell_type":"markdown","metadata":{"id":"JSWjka_U_Fhm","colab_type":"text"},"source":["\n","(a) **Unbiased and consistent**. The expectation of\n","    $\\widehat{p}$ is $(1/n)(np) = p$, and the variance converges to 0\n","    since $\\widehat{p}$ is an average of i.i.d., finite-variance\n","    random variables. Therefore, the mean squared error converges to 0\n","    as $n\\to\\infty$\n","    \n","    \n","(b) **Biased and consistent**. The estimator is biased\n","    because its value is always slightly smaller than the unbiased\n","    estimator (which has $n-1$ instead of $n$ in the denominator). The\n","    estimator is nevertheless consistent, since the bias and the\n","    variance both converge to 0 as $n\\to\\infty$.\n","\n","  \n","(c) **Unbiased and inconsistent**. The mean of $\\widehat{\\mu}$\n","    is $(1/100)(100 \\mu) = \\mu$, so the estimator is unbiased. The\n","    variance isn't zero and doesn't depend on $n$, so it cannot\n","    converge to 0 as $n\\to\\infty$. Therefore, the estimator is\n","    inconsistent.\n","    \n","    \n","(d) **Biased and inconsistent**. This estimator is\n","    inconsistent for the same reason as (c). The bias is trickier. Since\n","    the variance of $\\widehat{\\sigma}$ is positive, then we have\n","    $\\mathbb{E}[\\hat{\\sigma}^2]  -\\mathbb{E}[\\hat{\\sigma}]^2 >0$, which implies that \n","    $$\\mathbb{E}[\\hat{\\sigma}]^2<\\mathbb{E}[\\hat{\\sigma}^2] = \\mathbb{E}\\left[\\frac{1}{99}\\sum_{i=1}^{100}(X_i-\\overline{X})^2\\right]=\\sigma^2$$\n","    \n","Thus the bias of $\\widehat{\\sigma}$ is negative."]},{"cell_type":"markdown","metadata":{"id":"Ad5sqEW6_Fho","colab_type":"text"},"source":["## Problem 2\n","\n","Suppose that $X_1, \\dots, X_n$ are independent\n","  $\\mathrm{Unif}[0, \\theta]$ random variables, where $\\theta$ is an\n","  unknown parameter, and consider the\n","  following estimators for $\\theta$: \n","  $$\\widehat{\\theta}_1 = \\max(X_1, \\dots, X_n),  \\qquad \\widehat{\\theta}_2 = 2 \\cdot \\frac{X_1 + \\cdots + X_n}{n}$$\n","\n","\n","(a) Find the CDF of $\\widehat{\\theta}_1$. \n","\n","(b) Recall that if $F_{\\widehat{\\theta}_1}(x)$ and\n","    $f_{\\widehat{\\theta}_1}(x)$ are the CDF and PDF of\n","    $\\widehat{\\theta}_1$ respectively,\n","    then $\n","    \\displaystyle{\\frac{d}{dx}F_{\\widehat{\\theta}_1}(x) =\n","      f_{\\widehat{\\theta}_1}(x)}$.\n","    Differentiate your answer to (a) to find the PDF of $\\widehat{\\theta}_1$. \n","\n","(c) Show that $\\widehat{\\theta}_1$ is consistent. \n","\n","(d) Find $\\mathbb{E}\\left[{\\widehat{\\theta}_1}\\right]$ and\n","    $\\mathbb{E}\\left[\\widehat{\\theta}_2\\right].$ Which estimator is biased? \n","\n","(e) Find $\\operatorname{Var}\\left({\\widehat{\\theta}_1}\\right)$ and\n","    $\\operatorname{Var}\\left(\\widehat{\\theta}_2\\right).$ Which estimator has lower\n","    variance? \n","\n","(f) Show that the mean squared error of $\\widehat{\\theta}_1$\n","    is less than the mean squared error of $\\widehat{\\theta}_2$\n","    whenever $n \\geq 3$. \n","    \n","Hint: this problem is pretty calculus intensive. SymPy is your friend."]},{"cell_type":"markdown","metadata":{"id":"780IFTMK_Fhq","colab_type":"text"},"source":["\n","(a) The probability that $\\widehat{\\theta}_1$ exceeds $t \\in\n","    [0,\\theta]$ is the probability that all of the $X_i$'s are less\n","    than or equal to $t$. By independence, this probability is\n","    $(t/\\theta)^n$. Therefore,\n","    $$F_{\\widehat{\\theta}_1}(t) = \\begin{cases}\n","        0 & t \\leq 0 \\\\\n","        (t/\\theta)^n & 0 \\leq t \\leq \\theta \\\\\n","        1 & \\theta \\leq t \n","      \\end{cases}$$\n","      \n","(b) Differentiating $F_{\\widehat{\\theta}_1}(t)$ gives\n","    $nt^{n-1}/\\theta^n$.\n","    \n","(c) The probability that\n","    $\\widehat{\\theta}_1$ is less than $\\theta - \\epsilon$ is\n","    $$\\left(\\frac{\\theta - \\epsilon}{\\theta}\\right)^n,$$\n","   which converges to 0 as $n\\to\\infty$.\n","\n","(d) We have\n","    $$\\mathbb{E}[\\widehat{\\theta}_1] = \\int_0^\\theta t (nt^{n-1}/\\theta^n) \\,\n","      d\\theta = \\frac{n}{n+1}\\theta,$$\n","   and\n","    $$\\mathbb{E}[\\widehat{\\theta}_2] = 2\\mathbb{E}[X_1 + \\cdots + X_n] / n =\n","      2(n\\theta/2)/n = \\theta$$\n","   So $\\widehat{\\theta}_1$ is biased and $\\widehat{\\theta}_2$ is\n","    unbiased.\n","\n","(e) We have\n","    $$\\mathbb{E}[\\widehat{\\theta}_1] = \\int_0^\\theta t^2 (nt^{n-1}/\\theta^n) \\,\n","      d\\theta = \\frac{n}{n+2}\\theta^2,$$\n","   so the variance of $\\widehat{\\theta}_1$ is\n","    $$\\frac{n}{n+2}\\theta^2 - \\left(\\frac{n}{n+1}\\theta\\right)^2 =\n","      \\frac{n\\theta^2}{(n+1)^2(n+2)}.$$\n","   The variance of $\\widehat{\\theta}_2$ is\n","    $$\\operatorname{Var}(\\widehat{\\theta}_2) =\n","      \\frac{4}{n^2}\\operatorname{Var}(X_{1}+X_{2}+\\cdots+X_{n}) =\n","      \\frac{4\\sigma^2}{n} = \\frac{\\theta^2}{3n}.$$\n","\n","(f) The mean squared error of $\\widehat{\\theta}_2$ is its variance\n","    $\\frac{\\theta^2}{3n}$, since it is unbiased. The mean squared\n","    error of $\\widehat{\\theta}_1$ is\n","    $$\\frac{n\\theta^2}{(n+1)^2(n+2)} +\n","      \\left(\\frac{\\theta}{n+1}\\right)^2 =\n","      \\frac{2\\theta^2}{(n+1)(n+2)}.$$\n","   These expressions are equal when $n = 1$ and when $n = 2$, but the former is larger for all $n \\geq 3$. \n","   \n","   \n","(Some `SymPy` code for checking the calculations above:)"]},{"cell_type":"code","metadata":{"id":"dFpikskq_Fhq","colab_type":"code","colab":{}},"source":["\n","using SymPy\n","@vars θ t\n","@vars n integer=true positive=true\n","F = t^n/θ^n\n","f = diff(F,t)\n","μ = simplify(integrate(t*f,(t,0,θ)))\n","simplify(integrate(t^2*f,(t,0,θ)))\n","σ² = factor(integrate(t^2*f,(t,0,θ)) - μ^2)\n","(μ-θ)^2 + σ² |> simplify |> factor # returns 2θ²/((n+1)(n+2))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ut-8_S8V_Fhu","colab_type":"text"},"source":["## Problem 3\n","\n","(a) **Hoeffding's inequality** says that if\n","    $Y_1, Y_2, \\ldots$ are independent random variables with the\n","    property that $\\mathbb{E}[Y_i] = 0$ and $a_i \\leq Y_i \\leq b_i$ for all\n","    $i$, then for all $\\epsilon>0$ and $t > 0$, we have\n","    $$\\mathbb{P}\\left(Y_1 + Y_2 + \\cdots + Y_n \\geq \\epsilon \\right) \\leq\n","      e^{-t \\epsilon} \\prod_{i=1}^n e^{t^2(b_i-a_i)^2/8}.$$\n","    \n","   Use Hoeffding's inequality to show that if $X_1, X_2, X_3, \\ldots$\n","    is a sequence of independent $\\operatorname{Bernoulli}(p)$ random\n","    variables, then for all $\\alpha > 0$, the interval\n","    $\\left(\\overline{X}_n - \\sqrt{\\frac{1}{2n}\\log(2/\\alpha)},\n","      \\overline{X}_n + \\sqrt{\\frac{1}{2n}\\log(2/\\alpha)}\\right)$ is a\n","    confidence interval for $p$ with confidence level $1 -\n","    \\alpha$. Explain what happens to the width of this confidence\n","    interval if $n$ gets large, and also what happens to the width\n","    if $\\alpha$ is made very small.\n","    \n","(b) As above, consider $n$ independent\n","    $\\operatorname{Bernoulli}(p)$'s. Find the normal-approximation\n","    confidence interval for $p$\n","\n","(c) As above, consider $n$ independent\n","    $\\operatorname{Bernoulli}(p)$'s. Find the Chebyshev confidence\n","    interval for $p$. (Chebyshev's inequality says that the probability of any random variable deviating from its mean by more than $k$ standard deviations is no more than $1/k^2$.)\n","\n","(d) Find the numerical values of the half-widths for each of the\n","    above confidence intervals when $p = \\frac{1}{2}$, $n = 1000$, and\n","    $\\alpha = 0.05$ (approximating $\\overline{X}$ as $p$). "]},{"cell_type":"markdown","metadata":{"id":"Cz2qaMbw_Fhu","colab_type":"text"},"source":["\n","(a) Let's define $Y_i = (X_i - p)/n$. Then $\\mathbb{E}[Y_i] = 0$, and the\n","    the tightest interval $[a_i,b_i]$ that contains the range of $Y_i$\n","    is $[-p/n,(1-p)/n]$. So Hoeffding's inequality says that\n","    $$\\mathbb{P}(\\overline{X}_n - p \\geq \\epsilon) \\leq\n","      e^{-t\\epsilon+nt^2/(8n^2)}.$$\n","   Since this inequality holds for all $t$, we achieve the best upper\n","    bound by choosing the value of $t$ which minimizes the exponent on\n","    the right-hand side. Since the graph of that expression is a\n","    convex parabola, we can find the minimum of the expression by\n","    differentiating and finding the unique critical point. We find\n","    that the minimizing value of $t$ is $4\\epsilon/n$, which means\n","    that\n","    $$\\mathbb{P}(\\overline{X}_n - p \\geq \\epsilon) \\leq e^{-2n\\epsilon^2}.$$\n","   \n","   Substituting $\\epsilon_n = \\sqrt{\\frac{1}{2n}\\log(2/\\alpha)}$, we\n","    get $\\mathbb{P}(\\overline{X}_n - p \\geq \\epsilon_n) \\leq\n","    \\alpha/2$. Likewise, we can repeat all of the above for\n","    $Y_i = -(X_i - p)/n$ and find that\n","    $\\mathbb{P}(\\overline{X}_n - p \\leq -\\epsilon_n) \\leq \\alpha/2$. So the\n","    probability that $|\\overline{X}_n - p| \\geq \\epsilon_n$ is no more\n","    than $\\frac{\\alpha}{2} + \\frac{\\alpha}{2} = \\alpha$ (by\n","    the subadditivity property of probability measures).\n","\n","   As $n\\to\\infty$, the confidence interval shrinks, and if $\\alpha$\n","    is very small, then the confidence interval grows. Both of these\n","    are consistent with what you would expect: more data permits a\n","    tighter confidence interval, and a higher confidence level\n","    requires a wider confidence interval.\n","\n","(b) The normal-approximation confidence interval is\n","    $(\\overline{X}_n - z_{\\alpha/2}\\sqrt{\\overline{X}_n\n","      (1-\\overline{X}_n)/n}, \\overline{X}_n +\n","    z_{\\alpha/2}\\sqrt{\\overline{X}_n (1-\\overline{X}_n)/n})$, where\n","    $z_{\\alpha/2}$ is the value such that the standard normal\n","    distribution assigns mass $1-\\alpha$ to $[-z_{\\alpha/2},\n","    z_{\\alpha/2}]$.\n","\n","(c) The Chebyshev confidence interval is\n","    $(\\overline{X}_n - \\frac{1}{\\sqrt{\\alpha}}\\sqrt{\\overline{X}_n\n","      (1-\\overline{X}_n)/n}, \\overline{X}_n +\n","    \\frac{1}{\\sqrt{\\alpha}}\\sqrt{\\overline{X}_n\n","      (1-\\overline{X}_n)/n})$, where the expression\n","    $\\frac{1}{\\sqrt{\\alpha}}$ is obtained by solving the equation\n","    $1/k^2 = \\alpha$ for $k$.\n","\n","(e) We approximate $\\overline{X}_n \\approx p$ to find the values\n","    $$\\sqrt{\\log(2/0.05)/(2\\cdot1000)} \\approx 0.028\n","      \\quad\n","      1.96\\sqrt{(1/2)(1-1/2)/1000} \\approx 0.031\n","      \\quad\n","      \\frac{1}{\\sqrt{0.05}}\\sqrt{(1/2)(1-1/2)/1000} \\approx 0.071$$\n","   So we can see that the normal approximation provides the tightest\n","    confidence interval, while Hoeffding does better than Chebyshev."]},{"cell_type":"markdown","metadata":{"id":"HWcbRhoW_Fhv","colab_type":"text"},"source":["## Problem 4\n","\n","I drew 6 observations from an undisclosed distribution and obtained the following results:\n","  `u = [6.19,7.048,6.143,5.459,4.603,4.335]`\n","  \n","I also drew 8 observations from another undisclosed distribution and got \n","  `v = [8.924,4.698,6.095,4.223,3.643,1.624,1.444,6.309]`\n","  \n","(a) Determine whether the Wald hypothesis test (with significance $\\alpha = 0.05$) rejects the null hypothesis that the mean of the two distributions are equal. \n","\n","(b) Repeat with Welch's t-test in place of the the Wald test. "]},{"cell_type":"markdown","metadata":{"id":"z4GZD-LV_Fhv","colab_type":"text"},"source":["We estimate the standard error of the difference of sample means as "]},{"cell_type":"code","metadata":{"id":"8HqsoiTv_Fhv","colab_type":"code","colab":{}},"source":["using Statistics\n","se = sqrt(std(u)^2/length(u) + std(v)^2/length(v))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QJauIpr4_Fhx","colab_type":"text"},"source":["which returns $0.98$. Thus the observed difference between sample means is `(mean(u) - mean(v))/se` = $1.03$ standard deviations from the mean. Since $1.03 < 1.96$, we retain the null hypothesis. "]},{"cell_type":"markdown","metadata":{"id":"J4o9Y49y_Fhx","colab_type":"text"},"source":["## Problem 5\n","\n","Consider a distribution $\\nu$ which is known only via a dozen samples therefrom, the values of which are\n","```julia\n","    [8.924,4.698,6.095,4.223,3.643,1.624,1.444,6.309]\n","```\n","\n","(a) Obtain a bootstrap estimate of the standard deviation of the\n","    median of five independent samples from $\\nu$.\n","\n","(b) The actual standard deviation of the median of 5 samples from\n","    $\\nu$ is approximately 2.14. How close is the value you found?\n","    Could you have gotten as close as desired to this value by\n","    choosing sufficiently many bootstrap re-samplings?"]},{"cell_type":"markdown","metadata":{"id":"CVJaFq--_Fhy","colab_type":"text"},"source":["\n","(a) We calculate"]},{"cell_type":"code","metadata":{"id":"bUwazky1_Fhy","colab_type":"code","colab":{}},"source":["using Random, Statistics, StatsBase\n","X = [8.924,4.698,6.095,4.223,3.643,1.624,1.444,6.309]\n","std(median(sample(X,5)) for _ in 1:10^6)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7hhIeTEa_Fhz","colab_type":"text"},"source":["No, we could not get arbitrarily close to the correct value, because bootstrapping only allows us to approximate the plug-in estimator arbitrarily well. To get a better estimate of the actual value of the statistical functional, we would need more observations from the original distribution."]},{"cell_type":"markdown","metadata":{"id":"8XEKfFop_Fh0","colab_type":"text"},"source":["## Problem 6\n","\n","Consider a population of patients who have had a recent heart attack. A randomized trial is conducted in which each patient is assigned with equal probability (and independently of any attribute of the patient) to either a heart medication regimen or a placebo. Each patient has a unknown genetic attribute $X$ which is uniformly distributed on $[0,1]$.\n","\n","Suppose that the probability that a patient will comply with the regimen (that is, take the medication as prescribed) is $(1+X)/2$. The conditional probability that they will survive the following decade, given $X$ and given that they take the drug, is $3X/4$. The conditional probability that they will survive the following decade is, given $X$ and given that they do not take the drug, is $(X+1)/4$.\n","\n","(a) Fill out the following tables, indicating the probability of each outcome. The eight numbers should sum to 1. \n","\n","<table>\n","  <tr>\n","    <th>Drug condition</th>\n","    <td>survives</td>\n","    <td>does not survive</td>\n","  </tr>\n","  <tr>\n","    <td>complaint</td>\n","    <td>_</td>\n","    <td>_</td>\n","  </tr>\n","  <tr>\n","    <td>non-compliant</td>\n","    <td>_</td>\n","    <td>_</td>\n","  </tr>\n","</table>\n","<table>\n","  <tr>\n","    <th>Placebo condition</th>\n","    <td>survives</td>\n","    <td>does not survive</td>\n","  </tr>\n","  <tr>\n","    <td>complaint</td>\n","    <td>_</td>\n","    <td>_</td>\n","  </tr>\n","  <tr>\n","    <td>non-compliant</td>\n","    <td>_</td>\n","    <td>_</td>\n","  </tr>\n","</table>\n","\n","Hint: you want to work out the conditional probabilities of each event given $X$, and then find the expected value of the resulting conditional probability by integrating against the density of $X$. \n","\n","(b) Does a randomly selected patient have a higher conditional probability of surviving if they take the drug or if they do not? Does comparing the survival probabilities for the drug and placebo conditions give the correct answer to this question?\n","\n","(c) Suppose you know yourself well enough to be confident that you'd be relatively unlikely to comply with a prescription regimen if you had participated in this clinical trial. Based on the given probability model, should you take the drug? Would you get the right answer or the wrong answer if you just compared the survival rates for compliant and noncompliant patients?"]},{"cell_type":"markdown","metadata":{"id":"vMXXrRG8_Fh0","colab_type":"text"},"source":["\n","*Solution*.  \n","(a) We calculate the indicated values using SymPy: "]},{"cell_type":"code","metadata":{"outputHidden":false,"inputHidden":false,"id":"NFwP_28o_Fh0","colab_type":"code","colab":{},"outputId":"c501aa6e-440c-4054-b04c-b9e1ef9b0086"},"source":["drug_table"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2×2 Array{Sym,2}:\n","       3*x*(x/2 + 1/2)/4  (1 - 3*x/4)*(x/2 + 1/2)\n"," (1/2 - x/2)*(x/4 + 1/4)  (1/2 - x/2)*(3/4 - x/4)"],"text/latex":"\\[\\left[ \\begin{array}{rr}\\frac{3 x \\left(\\frac{x}{2} + \\frac{1}{2}\\right)}{4}&\\left(1 - \\frac{3 x}{4}\\right) \\left(\\frac{x}{2} + \\frac{1}{2}\\right)\\\\\\left(\\frac{1}{2} - \\frac{x}{2}\\right) \\left(\\frac{x}{4} + \\frac{1}{4}\\right)&\\left(\\frac{1}{2} - \\frac{x}{2}\\right) \\left(\\frac{3}{4} - \\frac{x}{4}\\right)\\end{array}\\right]\\]"},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"DXemknuU_Fh2","colab_type":"code","colab":{},"outputId":"ce5b9ef5-1e36-43a5-8ecd-ea684037c278"},"source":["using SymPy\n","@vars x\n","survival_given_compliance = 3x/4\n","survival_given_noncompliance = (x+1)/4\n","compliance_prob = (1+x)/2\n","\n","drug_table = \n","[ \n","  survival_given_compliance * compliance_prob        (1-survival_given_compliance) * compliance_prob \n","  survival_given_noncompliance * (1-compliance_prob) (1-survival_given_noncompliance) * (1-compliance_prob)\n","]\n","\n","integrate.(drug_table,Ref((x,0,1)))/2 # Ref protects the tuple from the dot, so it doesn't try to broadcast"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2×2 Array{Sym,2}:\n"," 5/32  7/32\n"," 1/24  1/12"],"text/latex":"\\[\\left[ \\begin{array}{rr}\\frac{5}{32}&\\frac{7}{32}\\\\\\frac{1}{24}&\\frac{1}{12}\\end{array}\\right]\\]"},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"solution":true,"id":"mroTHOzE_Fh3","colab_type":"code","colab":{}},"source":["placebo_table = \n","[ \n","  survival_given_noncompliance * compliance_prob        (1-survival_given_noncompliance) * compliance_prob \n","  survival_given_noncompliance * (1-compliance_prob) (1-survival_given_noncompliance) * (1-compliance_prob)\n","]\n","\n","integrate.(placebo_table,Ref((x,0,1)))/2"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OZ8WJ1Dh_Fh4","colab_type":"text"},"source":["(b) Yes, a randomly selected patient has a higher probability of surviving with the drug than without it. Comparing rates does give the correct answer. \n","\n","(c) No, you should not take the drug. If you know you're not inclined to be compliant, that's an indication that your value of $X$ is small. And the conditional probability of survival is larger without the drug when $X$ is small: "]},{"cell_type":"code","metadata":{"id":"su3dL2Zu_Fh5","colab_type":"code","colab":{}},"source":["using Plots, LaTeXStrings\n","plot(0:0.01:1, [x->3x/4, x->(x+1)/4], label = [\"drug\", \"no drug\"],\n","     xlabel=L\"X\", ylabel=\"survival probability\", leg = :topleft, ylims = (0,1))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BFf9z3hJ_Fh5","colab_type":"text"},"source":["## Problem 7\n","\n","(a) Write a function which a distribution `D`, together with an $\\alpha$ value and a positive integer $n$ and returns `true` or `false` according to whether the empirical CDF for a random sample of size $n$ obeys the bound in the DKW inequality. \n","\n","(b) Run the function many times with `α = 0.05` and check that it returns `true` around 95% of the time. "]},{"cell_type":"code","metadata":{"id":"m-vXBQrO_Fh6","colab_type":"code","colab":{}},"source":["function DKW_check(D, α, n)\n","    sample = sort(rand(D,n)) \n","    for i in eachindex(sample) \n","        ϵ = √(log(2/α)/(2n)) \n","        if max(abs(cdf(D, sample[i]) - i/n), abs(cdf(D, sample[i]) - (i-1)/n)) > ϵ \n","            return false \n","        end \n","    end \n","    true \n","end"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K_DA37Vg_Fh6","colab_type":"code","colab":{}},"source":["D = Uniform(0,1)\n","α = 0.05\n","n = 1000\n","mean(DKW_check(D, α, n) for _ in 1:10000)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IfEs6Mhu_Fh7","colab_type":"text"},"source":["## Problem 8\n","\n","Consider a family of distributions of the form $\\mathrm{Uniform}(\\theta, \\theta+1)$, where $\\theta$ is a parameter. Given a sample $X_1, \\ldots, X_n$, show that there isn't a unique maximum likelihood estimator for $\\theta$"]},{"cell_type":"markdown","metadata":{"id":"FXBp02rB_Fh8","colab_type":"text"},"source":["\n","The likelihood is constant for all $\\theta$ values for which $(\\theta, \\theta + 1)$ traps all of the observations. For other values of $\\theta$, the likelihood is zero.\n","\n","Since the sample range is necessarily strictly less than 1, there will be an interval of $\\theta$ values (from the sample maximum minus 1 up to the sample minimum) which have the same likelihood. "]},{"cell_type":"code","metadata":{"id":"qLO4fTkd_Fh8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}