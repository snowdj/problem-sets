{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["<style>\n", "@media print\n", "{\n", "h2 {page-break-before:always}\n", "}\n", "</style>\n", "\n", "# Homework 04\n", "\n", "### Brown University  \n", "### DATA 1010  \n", "### Fall 2019"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 1\n", " \n", "<img src=\"dartboard.png\" alt=\"drawing\" width=\"200\" align=right>\n", "Suppose that the probability density function for the random point where your dart hits the dartboard $D \\subset \\mathbb{R}^2$ is given by $$f(x,y) = \\frac{1}{\\pi} e^{-x^2 - y^2},$$ where the origin is situated at the dartboard\u2019s bull\u2019s eye, and where $x$ and $y$ are measured in inches (this function is positive everywhere in $\\mathbb{R}^2$, so the \"dartboard\" includes the disk shown as well as the (infinite) wall it is mounted on\u2014this is realistic insofar as one can indeed hit the wall with a dart throw). Find the probability of scoring triple 20 on your next throw. \n", "\n", "*(Optional)* Confirm your result using Monte-Carlo simulation.\n", "\n", "**Note:** The triple 20 region is the smaller of the two thin red strips in the sector labeled \u201c20\u201d. The inner and outer radii of this thin strip are 3.85 inches and 4.2 inches, respectively. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "The region in question is described most easily in polar coordinates: it is the set of points whose polar coordinates $(r,\\theta)$ satisfy $r_{i} \\leq r \\leq r_{o}$ and\\* $81^\\circ \\leq \\theta \\leq 99^\\circ$, where $r_{i} = 3.85$ and $r_{o} = 4.2$. (Note that the width of each sector is $360^\\circ/20 = 18^\\circ$, so the angles of the rays bounding the sector labeled 20 are $90^\\circ \\pm \\tfrac{18^\\circ}{2}$)\n", "\n", "Therefore, we can obtain the probability of hitting the triple 20 by expressing the density function in polar coordinates and integrating:\n", "$$\\int_{9\\pi/20}^{11\\pi/20} \\int_{r_{i}}^{r_{o}}\n", "    \\frac{1}{{\\pi}}e^{-r^2} r \\, d r \\, d\\theta =\n", "    \\left(\\frac{\\pi}{10}\\right) \\left(\\frac{1}{{\\pi}}\\right)\n", "    \\left(-\\tfrac{1}{2}e^{-r_{o}^2}-\\left(-\\tfrac{1}{2}e^{-r_{i}^2}\\right)\\right).$$\n", "\n", "Substituting the given values of $r_{i}$ and $r_{o}$ yields a probability of approximately $\\boxed{1.717\\times10^{-8}}$ of scoring 60\n", "on a single throw.   "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 2\n", "  \n", "Suppose that $X$ and $Y$ have joint PDF $f(x,y) = \\frac{3}{2}y$ on the upper unit disk (that is, the set of points which have positive $y$-coordinate and are less than one unit from the origin).\n", "\n", "1.  Verify that $f$ is indeed a probability density function.\n", "\n", "2.  Find the density of the distribution of $X$.\n", "\n", "3.  Find the conditional density of $Y$ given $X = x$.\n", "\n", "4.  Find $\\mathbb{E}[Y | X]$."]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "1.  We have $\\int_0^{\\pi} \\int_{0}^1 \\frac{3}{2}r\\sin \\theta (r \\,\n", "        d r \\, d \\theta) = 1$, so $f$ is indeed a probability density\n", "    function.\n", "\n", "2.  The distribution of $X$ is obtained by integrating out $y$:\n", "    $$\\int_{\\mathbb{R}} \\frac{3}{2}y \\boldsymbol{1}_{0 \\leq y \\leq\n", "            \\sqrt{1-x^2}} \\, d y = \\int_{0}^{\\sqrt{1-x^2}} \\frac{3}{2}y\n", "          \\, d y = \\frac{3}{4}(1-x^2).$$\n", "\n", "3.  The conditional density of $Y$ given $X = x$ is the joint density\n", "    divided by $X$\u2019s marginal density at $x$: $$f_{Y | X = x}(y) =\n", "          \\frac{f_{X,Y}(x,y)}{\\frac{3}{4}(1-x^2)} =\n", "          \\frac{\\frac{3}{2}y\\boldsymbol{1}_{0 \\leq y \\leq\n", "              \\sqrt{1-x^2}}}{\\frac{3}{4}(1-x^2)} =\n", "          \\frac{2y\\boldsymbol{1}_{0 \\leq y \\leq \\sqrt{1-x^2}}}{1-x^2}.$$\n", "\n", "4.  The conditional expectation of $Y$ given $X$ is obtained by\n", "    integrating $y$ times the conditional density of $Y$ given $X = x$\n", "    and then substituting $X$ for $x$:\n", "    $$\\int_{\\mathbb{R}}y\\frac{2y\\boldsymbol{1}_{0 \\leq y \\leq\n", "              \\sqrt{1-x^2}}}{1-x^2} \\, d y=\n", "          \\int_{0}^{\\sqrt{1-x^2}}\\frac{2y^2}{1-x^2} \\, d y =\n", "          \\frac{2(1-x^2)^{3/2}}{3(1-x^2)} = \\frac{2}{3}\\sqrt{1-x^2}.$$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 3\n", "\n", "The *skewness* of a distribution $\\nu$ is a measure of its asymmetry about its mean. It is defined to be\n", "$$\\mathbb{E}\\left[\\left(\\frac{X-\\mu}{\\sigma}\\right)^3\\right],$$ where $X$ is a random variable with distribution $\\nu$, $\\mu$ is the mean of $X$, and $\\sigma$ is the standard deviation of $X$. Find the skewness of the exponential distribution with parameter $1$. You should set up the integrals on your own, but feel free to evaluate them using a symbolic computation system.   "]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "We begin by calculating\n", "$$\\mathbb{E}[X] = \\int_0^\\infty x \\lambda e^{-\\lambda x} \\, d x =\n", "    1/\\lambda,$$ and\n", "$$\\mathbb{E}[X^2] = \\int_0^\\infty x^2 \\lambda e^{-\\lambda x} \\, d x =\n", "    2/\\lambda^2,$$ so $\\sigma = \\sqrt{\\mathbb{E}[X^2] - \\mathbb{E}[X]^2} = 1/\\lambda$.\n", "Finally, $$\\mathbb{E}\\left[\\left(\\frac{X - \\mu}{\\sigma}\\right)^3\\right] =\n", "    \\int_0^\\infty \\frac{(x-\\mu)^3}{\\sigma^3} \\lambda e^{-\\lambda x} \\, d x\n", "    = 2$$ \n", "    \n", "Evaluating these integrals using `SymPy`:\n", "\n", "```julia\n", "using SymPy\n", "@vars x \u03bb\n", "integrate(x*\u03bb*exp(-\u03bb*x),(x,0,oo))\n", "integrate(x^2*\u03bb*exp(-\u03bb*x),(x,0,oo))\n", "integrate((x-1/\u03bb)^3/(1/\u03bb)^3*\u03bb*exp(-\u03bb*x),(x,0,oo))\n", "```"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 4\n", "\n", "Let $X$ be the first digit of the number of residents of a randomly selected world city. What would you expect the distribution of $X$ to look like? What about the *last* digit $Y$?\n", "\n", "Load the associated world city populations CSV as a DataFrame and check your predictions. Compare to the distribution with probability mass function\n", "$$m(d) = \\log_{10}(d+1) - \\log_{10}(d) \\quad \\text{ for }d \\in \\{1,2,\\ldots,9\\}.$$\n", "\n", "```julia\n", "using StatsBase, Plots, FileIO, DataFrames\n", "D = DataFrame(load(\"cities.csv\"))\n", "D[:Population]\n", "tallydict = # you fill in this part\n", "sticks(1:9,collect(values(tallydict)))\n", "```"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "One natural prediction is that both distributions should be uniform (on 1:9 for the first digit, 0:9 for the last). Let\u2019s investigate.\n", "\n", "\n", "<img src=\"first-digits.png\" alt=\"drawing\" width=\"200\" align=right>\n", "\n", "```julia\n", "using StatsBase, Plots, FileIO, DataFrames\n", "D = DataFrame(load(\"cities.csv\"))\n", "D[:Population]\n", "tallydict = sort(countmap([string(n)[1] for n in D[:Population]]))\n", "ys = collect(values(tallydict))\n", "sticks(0:9,ys/sum(ys),label=\"first digit proportions\")\n", "sticks!((1:9).+0.1,[1/9 for d=1:9],label=\"uniform\")\n", "```\n", "\n", "This not seem like a good fit. Let\u2019s try the distribution suggested in the problem statement.\n", "\n", "<img src=\"first-digits-benford.png\" alt=\"drawing\" width=\"200\" align=right>\n", "\n", "```julia\n", "sticks(1:9,ys/sum(ys),label=\"first digit proportions\")\n", "sticks!((1:9).+0.1,[log10(d+1)-log10(d) for d=1:9],label=\"Benford\")\n", "```\n", "\n", "This fit seems better. This distribution is called <a href=\"https://en.wikipedia.org/wiki/Benford%27s_law\">Benford\u2019s distribution</a>, and it fits a variety of real-world leading-digit data, for reasons that are not completely well understood.\n", "\n", "Now, let's plot the distribution of the last digit:\n", "\n", "<img src=\"last-digits.png\" alt=\"drawing\" width=\"200\" align=right>\n", "\n", "```julia\n", "tallydict = sort(countmap([string(n)[end] for n in D[:Population]]))\n", "ys = collect(values(tallydict))\n", "sticks(0:9,ys/sum(ys),label=\"last digit proportions\")\n", "sticks!((0:9).+0.1,[1/10 for d=0:9],label=\"uniform\")\n", "```\n", "\n", "We can see that our prediction was pretty accurate for the last digit, except that this data set contains quite a few rounded numbers which causes 0 to be overrepresented."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 5\n", "\n", "A **call option** is a financial contract between two parties which grants the buyer the right, but not the obligation, to purchase a specified security at a specified price (called the **strike price**) at a specified date in the future (called the **expiration date**).\n", "\n", "Suppose that you purchase a call option for $10$ shares of AAPL with a strike price of $\\$216$ and an expiration $22$ business days from now. Suppose that the daily change in the price of AAPL is normally distributed with mean zero and standard deviation $\\$8.44$, and that the changes for different days are independent.\n", "\n", "(a) Find a function $f$ such that the call option is worth $f(P)$ dollars to you if the share price in 22 days is $P$. Draw a graph of $f$. Hint: if the price is greater than $\\$216$, would you exercise the option and purchase the stock? What if it\u2019s less than $\\$216$?\n", "\n", "(b) Find the distribution of $P$.\n", "\n", "(c) Find the fair price of the call option, based on the above assumptions.\n", "\n", "Notes: (1) the data in this problem are real: the current price at time of writing is $\\$216$, and the daily fluctuations have had an empirical standard deviation of $\\$8.44$ historically. The number of business days in a month is approximately $22$. (2) Although this problem uses finance ideas, all of the finance information you need to solve the problem is in the problem statement.\n", "\n", "To help you with the symbolic integration, here's some code for finding the expected value of the Gaussian distribution centered at \u03bc. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["using SymPy\n", "@vars P \u03c3 \u03bc positive=true\n", "I = integrate(P*1/sqrt(2*PI*\u03c3^2)*exp(-(P-\u03bc)^2/(2\u03c3^2)),(P,-oo,oo))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "(a) We have $f(P) = \\textrm{max}(0, P \u2212 216)$, since if the price is in excess of $\\$216$, we can sell it and make a profit equal to the difference between the price and $\\$216$. If the price is less, we would not exercise the option and it would be worthless.\n", "\n", "(b) The distribution of the price of the stock in 22 days is Gaussian with mean 216 and variance $\\$8.44 \u00b7 22 = \\$185.68$.\n", "\n", "(c) The expected value of the option is\n", "\n", "$$\n", "\\int_{-\\infty}^{\\infty}f(P)\\phi(P)\\,dP,\n", "$$\n", "\n", "where $\\phi(P)$ is the Gaussian density with mean 0 and variance 185.68. The code block\n", "\n", "```julia\n", "using SymPy\n", "@vars P \u03c3 \u03bc positive=true\n", "I = integrate((P-\u03bc)*1/sqrt(2*PI*\u03c3^2)*exp(-(P-\u03bc)^2/(2\u03c3^2)),P,\u03bc,oo)\n", "```\n", "\n", "returns $\\frac{\\sigma}{\\sqrt{2\\pi}}$, so we can say that the fair price of the option is $\\frac{\\sqrt{185.68}}{\\sqrt{2\u03c0}} \\approx \\$5.44$ dollars. (Note that we could alternatively evaluate this integral by hand using substitution)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 6\n", "\n", "Consider two random variables $X$ and $Y$ whose joint distribution has probability mass of $\\frac{1}{n}$ at each of the $n$ points\n", "$\\{(x_1, y_1), \\ldots, (x_n,y_n)\\}$ in $\\mathbb{R}^2$. Show that the covariance matrix of $X$ and $Y$ is equal to\n", "  $$\n", "    \\frac{1}{n}\\sum_{i=1}^{n}\\left[\\begin{smallmatrix}x_i-\\overline{x}\n", "        \\\\\n", "        y_i-\\overline{y}\\end{smallmatrix}\\right]\\left[\\begin{smallmatrix}\n", "        x_i-\\overline{x}\n", "    \\quad y_i-\\overline{y}\\end{smallmatrix} \\right]. \n", "  $$\n", "where $\\overline{x} = (x_1+\\cdots+x_n)/n$ and  $\\overline{y} = (y_1+\\cdots+y_n)/n$. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "The off-diagonal entries of the covariance matrix are each equal to\n", "  $$\n", "    \\mathbb{E}[(X-\\mu_X)(Y-\\mu_Y)]\n", "  $$\n", "  \n", "where $\\mu_X$ and $\\mu_Y$ are the expected values of $X$ and $Y$. Then using the formula $\\mathbb{E}[g(X,Y)] = \\sum_{(x,y) \\in \\mathbb{R}^2} g(x,y) m_{X,Y}(x,y)$, we find that\n", "\n", "  $$\n", "    \\mathbb{E}[XY] = \\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\overline{x})(y_i-\\overline{y}), \n", "  $$\n", "\n", "Meanwhile, the off-diagonal entry of $\\frac{1}{n}\\sum_{i=1}^{n}[x_i-\\overline{x}, y_i-\\overline{y}][x_i-\\overline{x}, y_i-\\overline{y}]'$ is\n", "\n", "  $$\n", "    \\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\overline{x})(y_i-\\overline{y}), \n", "  $$\n", "\n", "which is indeed equal to the expression we found for $\\mathbb{E}[XY]$. Similar analysis applies to the diagonal entries. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 8\n", "\n", "The *Epanechnikov* kernel is defined by\n", "$$\n", "    D(u) = \\frac{3}{4}(1-u^2)\\boldsymbol{1}_{|u| \\leq 1}. \n", "$$\n", "* Is $D$ continuous? Is it differentiable? Is it twice differentiable?\n", "* Is the tri-cube weight function continuous? Is it differentiable? Is it twice differentiable? \n", "  \n", "Feel free to use SymPy to perform the symbolic differentiation in this problem. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["We calculate derivatives of the polynomial expressions in the two\n", "  functions: \n", "```julia\n", "    using SymPy\n", "    @vars u\n", "    subs(diff(1-u^2,u),u=>1) \n", "    subs(diff((1-u^3)^3,u),u=>1)\n", "    subs(diff((1-u^3)^3,(u,2)),u=>1)\n", "```\n", "We find that the Epanechnikov kernel is continuous but not differentiable at $1$, since its derivative from the right is zero and its derivative from the left is negative. The tri-cube weight function is twice differentiable, since its first and second derivatives from the left are zero at 1. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 9\n", "\n", "Simulate $n = 1000$ samples from the joint distribution of $X$ and $Y$, given that $X$ is uniform on $[0,1]$ and $Y = 2 + 1.2X + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,0.5)$. Record the integrated squared error for the Nadaraya-Watson estimator (with bandwidth selected by cross-validation) and for the line of best fit.\n", "\n", "Notes: (1) You can approximate the integrated squared difference between two functions by evaluating the squared difference at the points of a fine-mesh grid. And (2) you'll have to write code for simulating from the joint distribution of $X$ and $Y$, but then the Nadara-Watson part you can mostly get from Data Gymnasia."]}, {"cell_type": "markdown", "metadata": {}, "source": ["We begin by loading the optimization package, defining the regression function, and defining a function to draw samples from the given distribution. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["using Optim\n", "r(x) = 2 + 1.2x\n", "function sampleXY()\n", "    X = rand()\n", "    Y = r(X) + sqrt(0.5)*randn()\n", "    (X,Y)\n", "end\n", "\n", "n = 1000\n", "samples = [sampleXY() for i=1:n]\n", "xs = 0:1/2^8:1\n", "ys = 0:1/2^5:6"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Next we do kernel density estimation with cross validation. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["D(u) = abs(u) < 1 ? 70/81*(1-abs(u)^3)^3 : 0 # tri-cube function\n", "D(\u03bb,u) = 1/\u03bb*D(u/\u03bb) # scaled tri-cube\n", "K(\u03bb,x,y) = D(\u03bb,x) * D(\u03bb,y) # kernel\n", "kde(\u03bb,x,y,samples) = sum(K(\u03bb,x-Xi,y-Yi) for (Xi,Yi) in samples)/length(samples)\n", "\n", "function kdeCV(\u03bb,i,samples)\n", "    x,y = samples[i]\n", "    newsamples = copy(samples)\n", "    deleteat!(newsamples,i)\n", "    kde(\u03bb,x,y,newsamples)\n", "end\n", "\n", "# first line approximates \u222bf\u0302\u00b2, the second line approximates -(2/n)\u222bf\u0302f\n", "J(\u03bb) = sum([kde(\u03bb,x,y,samples)^2 for x=xs,y=ys])*step(xs)*step(ys) - \n", "     2/length(samples)*sum(kdeCV(\u03bb,i,samples) for i=1:length(samples))\n", "\u03bb_best_cv = optimize(\u03bb->J(first(\u03bb)),[1.0],BFGS()).minimizer[1]\n", "r\u0302(\u03bb,x) = sum(D(\u03bb,x-Xi)*Yi for (Xi,Yi) in samples)/sum(D(\u03bb,x-Xi) for (Xi,Yi) in samples)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Finally, we approximate the integrated squared error for the nonparametric method as well as for the parametric method.       "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ISE_nonparametric = sum((r\u0302(\u03bb_best_cv,x) - r(x))^2 for x in xs)\n", "\n", "X = [ones(length(samples)) [x for (x,y) in samples]]\n", "\u03b2 = (X'*X) \\ X' * [y for (x,y) in samples]\n", "ISE_linear = sum((\u03b2\u22c5[1,x]-r(x))^2 for x in xs)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We find that the integrated squared error is much lower for the linear approximation, which makes sense because the regression function is in fact linear. In other words, the inductive bias of the model aligns well with actual probability measure, and that leads to increased accuracy relative to a model with less inductive bias. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 10\n", "\n", "* Find the variance of the uniform distribution on the interval $[0,10]$.\n", "* Generate 10 independent samples from the uniform distribution, calculate the average $\\overline{X}$ for those samples, and estimate the variance as $\\widehat{V} = \\frac{1}{10}\\sum_{i=1}^{10} (X_i-\\overline{X})^2$. Package this whole process as a function, and call it a million times to find the mean of $\\widehat{V}$.\n", "* Which is larger, the answer to (a) or the answer to (b)? Calculate the percent error. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["We package the sampling procedure as a function and call it $M$ times for $M = 10^6$: \n", "``` julia\n", "function variance_estimate(n)\n", "    X = [10*rand() for i=1:n]\n", "    X\u0304 = mean(X)\n", "    1/n * sum((x - X\u0304)^2 for x in X)\n", "end\n", "M = 10^6\n", "variance_estimate_mean = mean(variance_estimate(10) for i=1:M)\n", "true_variance = 10^2 / 12\n", "perc_error = (variance_estimate_mean - true_variance)/true_variance\n", "```\n", "We find that the variance estimate is lower than the true variance, with approximately $10\\%$ error. If we repeat for other values of $M$, we find that this percent error is not diminishing."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 11\n", "\n", "The Student\u2019s *t*-distribution with parameter $\u03bd$ is the distribution of the random variable\n", "\n", "$$\\frac{\\bar{X}_{n} - \\mu}{S_{n}/\\sqrt{n}}$$\n", "\n", "where $n = \u03bd + 1$, where $X_{1},...,X{n}$ is a sequence of independent $N(\\mu,\\sigma^{2})$\u2019s, where $\\bar{X} = \\frac1n (X_{1} + ... + X_{n})$, and where $S_{n} = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}(X_{i} - \\bar{X}_{n})^{2}}$\n", "\n", "Estimate the variance of the Student\u2019s *t*-distribution with parameter $\u03bd = 10$ by using the above description to sample\n", "from it $M$ times for some large $M$. Then compute the variance of the distribution which places probability mass $1/M$\n", "at each of the simulated samples.\n", "\n", "Look up the exact formula for the variance of the Student\u2019s *t*-distribution on Wikipedia and check that your result is\n", "close to the true value."]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "We write a function which samples from the given distribution a large number of times. For convenience, we take $\u03bc = 0$, $\u03c3 = 1$.\n", "\n", "```julia\n", "function sampleT(\u03bd=10)\n", "    n = \u03bd + 1\n", "    X = randn(n)\n", "    X\u0304 = mean(X)\n", "    S = \u221a(sum((x - X\u0304 )^2 for x in X)/(n-1))\n", "    X\u0304 /(S/\u221a(n))\n", "end\n", "M = 10^6\n", "samples = [sampleT() for i=1:M]\n", "m = mean(samples)\n", "sum((s - m)^2 for s in samples)/M\n", "```\n", "\n", "The formula we discover on Wikipedia is $\u03bd/(\u03bd \u2212 2)$, which is very close to the value obtained by our Monte Carlo\n", "simulation (approximately 1.25)."]}], "metadata": {"jupytext": {"formats": "ipynb,md"}, "kernelspec": {"display_name": "Julia 1.2.0", "language": "julia", "name": "julia-1.2"}, "language_info": {"file_extension": ".jl", "mimetype": "application/julia", "name": "julia", "version": "1.2.0"}}, "nbformat": 4, "nbformat_minor": 4}