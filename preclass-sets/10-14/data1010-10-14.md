---
jupyter:
  jupytext:
    formats: ipynb,md
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.2'
      jupytext_version: 1.6.0
  kernel_info:
    name: julia-1.2
  kernelspec:
    display_name: Julia 1.5.0
    language: julia
    name: julia-1.5
---

# Statistics Review: Point Estimation & Confidence Intervals

#### *4 October 2019*
#### *DATA 1010*

```julia
using Plots, Distributions
```

The central problem of statistics is to make inferences about a population or data-generating process based on the information in a finite sample drawn from the population. Last class we talked about kernel density estimation, which directly estimates the distribution. Today we'll talk about estimating values which boil the distribution down to a single number.


---

**Point estimation** is the inference of a single real-valued feature of the distribution of the data-generating process (such as its mean, variance, or median).


## Problem 1


Run the code below to define a function called `mysteryRV` which can be called (with no arguments) to sample from an unknown distribution. Also defined is a function `check_mean` which can be called with a single argument to check whether the supplied value is close to the mean of the underlying distribution. Figure out how to ascertain the mean accurately enough to get `check_mean` to respond affirmatively. Repeat with `check_var` (also defined in "mystery-distribution.jl").

```julia
include("mystery-distribution.jl")
```

```julia
histogram([mysteryRV() for _ in 1:1_000_000], label="mystery observation", normed = true)
```

```julia
mysteryRV()
```

```julia
mean(mysteryRV() for _ in 1:1_000_000)
```

```julia
check_mean(2.29)
```

```julia
mean((mysteryRV()-2.3)^2 for _ in 1:100_000)
```

```julia
check_var(1.42)
```

---

A **statistical functional** is any function $T$ from the set of distributions to $[-\infty,\infty]$. An **estimator** $\widehat{\theta}$ is a random variable defined in terms of $n$ i.i.d. random variables, the purpose of which is to approximate some statistical functional of the random variables’ common distribution. 

*Example: Suppose that $T(\nu)$ = the mean of $\nu$, and that $\widehat{\theta} = \frac{(X_1 + \cdots + X_n)}{n}$.*


## Problem 2

(a) Suppose that $X$ is an integer-valued random variable, and that $T$ is defined to the units digit of $X$. Is $T$ a statistical functional? 

(b) Suppose that for any probability measure $\nu$ on the plane, $T(\nu)$ is defined to be the expected squared distance between two points drawn independently from $\nu$. Is $T$ a statistical functional? Is $\widehat{\theta}(\mathbf{X}_1, \ldots, \mathbf{X}_n) = |\mathbf{X}_1|$ an estimator of $T$?

(c) Come up with your own statistical functional (unique enough that it can't be boiled down to a single word). 


# worksheet
*Solution*. 


# solution
*Solution*. (a) No, $T$ is a random variable. A statistical functional maps a distribution to a number, not an element of $\Omega$ to a number.  
(b) Yes, $T$ is a statistical functional, and $\widehat{\theta}$ is technically an estimator (but not a good one).  
(c) Lots of options here, e.g., expected squared difference between the 3rd and 7th largest values obtained when sampling from $\nu$ independently 10 times (assuming $\nu$ is a measure on the number line). 


---

## Problem 3

The **empirical measure** associated with a given set of observations is the discrete probability measure which assigns a probability mass of $\frac{1}{n}$ to the location of each of the observations in the sample. 

The **plug-in estimator** of a statistical functional $T$ is the random variable $T(\widehat{\nu})$, where $\widehat{\nu}$ is the empirical measure. 

(a) Is the empirical measure random (in other words, if we collected a fresh set of observations, would we get the same measure)? Is the underlying probability measure used to generation the observations random?  
(b) Why is the empirical measure useless when it comes to estimating the conditional expectation of $Y$ given $X$, assuming that the joint distribution of $X$ and $Y$ is represented by a density function?  
(c) Is the sample mean a plug-in estimator?


# worksheet
*Solution*. 


# solution
*Solution*. (a) Yes, the empirical measure is random, since we're placing masses at the random locations of the observations.  
(b) It's useless because most vertical lines do not pass through any of the observations. Even the ones that do only intersect a single point, so the results will be very noisy.  
(c) Yes, the sample mean is the plug-in estimator for the mean. 


### Bias

Given a distribution $\nu$ and a statistical functional $T$, let $\theta = T(\nu)$. The **bias** of an estimator of $\theta$ is the difference between the estimator’s expected value and $\theta$.

*Example: The expectation of the sample mean $\widehat{\theta} = \frac{(X_1 + \cdots + X_n)}{n}$ is $\frac{\mathbb{E}(X_1 + \cdots + X_n)}{n} = \mathbb{E}[\nu],$ so the bias of the sample mean is zero.*


## Problem 4

Compute the sample mean for five observations from the uniform distribution on $[0,1]$. Repeat a million times and make a histogram of the resulting million sample mean values. Does the sample mean appear to be unbiased?


# worksheet
*Solution*. 

```julia
# solution
histogram([mean(rand() for _ in 1:5) for _ in 1:10^6], normed = true, label = "")
```




## Problem 5

Compute the sample maximum (that is, the plug-in estimator for the distribution maximum) for five observations from the uniform distribution on $[0,1]$. Repeat a million times and make a histogram of the resulting million sample maximum values. Does the sample mean appear to be unbiased?


# worksheet
*Solution*. 


# solution
*Solution*. Yes, it's definitely biased: 

```julia
# solution
histogram([maximum(rand() for _ in 1:5) for _ in 1:10^6], normed = true, label = "")
```

The **standard error** of an estimator is its standard deviation.  


## Problem 6

Estimate the standard error of the sample maximum estimator (same as the previous problem: five observations from the uniform distribution on [0,1]). 

```julia
sample_maxes = [maximum(rand(5)) for _ in 1:1_000_000]
mean(sample_maxes)
sqrt(mean((x - 5/6)^2 for x in sample_maxes))
```

```julia
var(maximum(rand() for _ in 1:5) for _ in 1:10^6)
```

```julia
check_var_sample_max(0.0198)
```

```julia
# solution
using SymPy
@vars x
F = x^5
f = diff(F,x)
#plot(0:0.1:1, f, label = "density", legend = :topleft)
μ = integrate(x*f,(x,0,1))
integrate((x-μ)^2*f,(x,0,1))
```

# worksheet
*Solution.*


An estimator is **consistent** if $\widehat{\theta} \to \theta$ in probability as $n\to\infty$. This happens if and only if both the bias and the standard error go to zero as $n\to\infty$.


---
# Confidence Intervals


## Problem 7

(a) Approximate the mean of the distribution which generated the values stored in the vector `mysample`. 

```julia
mysample
```

```julia
mean(mysample)
```

(b) If one were to propose that the mean of the underlying distribution that generated these data is 2, would you find that implausible? How about 6?


# solution
*Solution*. 


---

**Confidence intervals** are estimators with error bars. In other words, rather than returning a single value as our estimator, we return an entire interval together with a confidence level $1-\alpha$. We're saying that the probability that that interval traps the true value of the statistical functional is at least $1-\alpha$.



## Problem 8

The distribution that the entries of `mysample` were drawn from is normal with unknown mean $\mu$ and variance $\sigma^2$. The distribution of the sample mean is therefore also normal. Use this information find a 95% confidence interval for the mean $\mu$.

```julia
mysample
```

# worksheet
*Solution*. 

```julia
mean(mysample), std(mysample)
```

```julia
μ̂, σ̂, n = mean(mysample), std(mysample), length(mysample)
```

```julia
(μ̂ - 1.96σ̂/√(n), μ̂ + 1.96σ̂/√(n))
```

# solution
*Solution*. The value of the sample mean is 1.93, and the estimated variance is 5.22. Therefore, a 95% confidence interval would extend from $1.93 - 1.96\cdot \sqrt{5.22}/10$ to $1.93 + 1.96\cdot \sqrt{5.22}/10$. 

```julia
# solution
mean(mysample) - 1.96std(mysample)/sqrt(10), mean(mysample) + 1.96std(mysample)/sqrt(10)
```

---

## Problem 9

Write a function which accepts a vector as an argument, returns a 95\% confidence interval for the mean of the distribution that the observations in the vector were drawn from (using the normal approximation). Run the function `traps_mean` several times to see whether your confidence interval does trap the true mean about 95% of the time.

```julia
function confidence_interval_mean(X)
    μ̂, σ̂I , n = mean(X), std(X), length(X)
    (μ̂ - 1.96σ̂/√(n), μ̂ + 1.96σ̂/√(n))
end
```

```julia
traps_mean(confidence_interval_mean)
```

---

## Problem 10

Write a function which accepts a vector as an argument, returns a 95\% confidence interval for the maximum of the distribution that the observations in the vector were drawn from (using the assumption that the distribution is uniform on $[0,b]$ for some unknown $b$.). Run the function `traps_max` several times to see whether your confidence interval does trap the true mean about 95% of the time.

```julia
function confidence_interval_max(X)
    M, n = maximum(X), length(X)
    (M, M/0.05^(1/n))
end
```

```julia
traps_max(confidence_interval_max)
```

```julia
# solution
function confidence_interval_max(X)
    b̂, n = maximum(X), length(X)
    (b̂, b̂ / 0.05^(1/n))
end
```

```julia
# solution
traps_max(confidence_interval_max)
```

## Problem 11

Write your own function `traps_mean` which accepts three arguments: 
* `confidence_interval`: a function which takes a vector of observations and returns a confidence interval for the mean of the distribution the generated the observations
* `D`: a distribution (from the package `Distributions`, like `Uniform(0,1)`, or `Poisson(3)`, etc.)
* `n`: number of observations
`traps_mean` should return `true` if the mean of the distribution (`mean(D)`) is in the confidence interval and `false` if not.

`traps_mean` should print the confidence interval and the true mean, and it should return `true` or `false` depending on whether the confidence interval does trap the mean. 

```julia
function traps_mean(confidence_interval, D, n)
    sample = rand(D,n) # solution
    a,b = confidence_interval(sample) # solution
    μ = mean(D) # solution
    println("Confidence interval: (", a, ",", b,")")
    println("Actual mean: ", μ)
    a ≤ μ ≤ b # solution
end

traps_mean(confidence_interval_mean, Uniform(3,7), 10)
```

```julia

```
