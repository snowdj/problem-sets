{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Julia 1.5.1","language":"julia","name":"julia-1.5"},"language_info":{"file_extension":".jl","mimetype":"application/julia","name":"julia","version":"1.5.1"},"colab":{"name":"data1010-hw-10-sol.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"Qh2IPF4Mv9E9"},"source":["# Homework 10\n","## Brown University\n","## DATA 1010\n","## Fall 2020"]},{"cell_type":"markdown","metadata":{"id":"XegyKzoekSmS"},"source":["## Problem 1"]},{"cell_type":"markdown","metadata":{"id":"dethKlPEkSmS"},"source":["Go to `https://prismia.chat/gallery` and check out the decision tree manipulative (this is the same one we used in class).\n","\n","(a) Drag all of the red points into the middle and the blue points equally split to the left and right (so that, left to right in order, you encounter five blues, then 10 reds, and then the other five blues). The projections of the points onto a vertical axis should be all mixed up. Does a greedily-trained decision tree end up with training accuracy of 100% for this example? (Note: you should think about the full CART algorithm, where there is no restriction on whether each split is horizontal or vertical, rather than the restricted one in the manipulative which only allows vertical splits at the first stage and horizontal ones at the second.)"]},{"cell_type":"markdown","metadata":{"id":"Ep-gBBRukSmS"},"source":["Yes, this works. Even though the initial split isn't that great (since it doesn't split the red and blues on the side with 3/4 of the points), it fortunately is still the *best* split, so the second split has a chance to come and split the reds in the middle from the other half of the blues. We end up with pure leaves after only two splitting operations."]},{"cell_type":"markdown","metadata":{"id":"OsWh0aHCkSmS"},"source":["(b) Come up with a configuration of the 10 red and 10 blue points where you can achieve 100% training accuracy (and where you would also anticipate having a high test accuracy as well), but where the greedy algorithm gives worse results."]},{"cell_type":"markdown","metadata":{"id":"zVCXlc4YkSmS"},"source":["Put red points in the even-numbered quadrants and blue points in the odd. The splits along the axes do a perfect job, but the greedy algorithm doesn't want to put a split on an axis because the goodness of the split doesn't become apparent until the second split occurs. "]},{"cell_type":"markdown","metadata":{"id":"vnKiNSoxkSmS"},"source":["## Problem 2"]},{"cell_type":"markdown","metadata":{"id":"7Ey7CdoPkSmS"},"source":["Go to `https://prismia.chat/gallery` and check out the neural network manipulative (this is the same one we used in class).\n","\n","(a) Starting from the good configuration of weights, biases, and input coordinates (click 'show best' to get that), move the middle bias dial all the way to the left (middle neuron in the middle column). Wiggle the sliders which govern the weights of the edges feeding into that neuron. What are the derivatives of the predicted gold probabilty (post-softmax) with respect to those two weights?"]},{"cell_type":"markdown","metadata":{"id":"rKzY4jy9kSmS"},"source":["The derivatives are zero, since that neuron is saturated at zero, and thus its value doesn't change when the input weights are changed slightly. As a result, nothing subsequent to that neuron in the computational graph changes either."]},{"cell_type":"markdown","metadata":{"id":"rUpeD8ShkSmS"},"source":["(b) Suppose that both biases in the last layer are both increased by the same amount. How does that affect the resulting (binary) prediction function?"]},{"cell_type":"markdown","metadata":{"id":"3uDSaTEKkSmS"},"source":["The prediction function is the same, since the prediction function's output is determined entirely by which of those two neurons has a larger value. Changing the values in any order-preserving way leaves the prediction function unchanged."]},{"cell_type":"markdown","metadata":{"id":"967ryJmukSmS"},"source":["(c) Suppose you wanted to have the same binary prediction function as the one shown when you click \"show best\", but you wanted the model's *probability* predictions to soften. In other words, you'd rather that the model go less quickly from nearly 100% gold to nearly 100% purple as you move across from one side of the semicircular boundary to the other.\n","\n","What change could you make to the weights and biases of the network to achieve this effect?"]},{"cell_type":"markdown","metadata":{"id":"V-3CIgEdkSmS"},"source":["We could multiply the last-layer weights and biases by a fixed constant $c$ less than 1, because that is equivalent to replacing the exponential function in the softmax calculation with $x \\mapsto \\exp(cx)$, and that produces less exaggerated differences. "]},{"cell_type":"markdown","metadata":{"id":"y3GA4FjekSmS"},"source":["## Problem 3"]},{"cell_type":"markdown","metadata":{"id":"I-Gt_qFGkSmS"},"source":["We say that a model with a deterministic training process is **scale-invariant** if the following two procedures yield the same predictions for each row of the data frame of test data:\n","\n","1. train the model on the training data\n","2. evaluate the model at each test input value to obtain test predictions\n","\n","and\n","\n","1. multiply a column of the data frame of training data by a positive number\n","2. train the model on the new training data, with the modified column\n","3. multiply that same column of the data frame of *test* data by that same positive number\n","4. evaluate the model at each new test input value to obtain test predictions"]},{"cell_type":"markdown","metadata":{"id":"F0PMzwOnkSmS"},"source":["Which of the models we've studied in this course are scale invariant? Be sure to address linear regression for regression problems, logistic regression for binary classification, soft- and hard-margin suppor vector machines for binary classification, and decision trees for classification or regression."]},{"cell_type":"markdown","metadata":{"id":"s7z3oGN8kSmS"},"source":["Linear regression is scale-invariant, since the model can and will adjust the coefficient of the scaled feature by a factor equal to the reciprocal of the scaling factor. Same thing with logistic regression. \n","\n","Support vector machines are not scale invariant, which can be seen by considering an extreme example where the units of one column make the values in that column vastly larger than any other. The decision boundary is forced to prioritize separation in that dimension over all other dimensions, leading to different results in general than if that scaling were not present (or, for example, if a different feature were heavily scaled).\n","\n","Decision trees are scale invariant, since only the ordering of values in each column is ever considered in the training process. Scaling the features doesn't change anything. "]},{"cell_type":"code","metadata":{"id":"pISb9J4EkSmS"},"source":[""],"execution_count":null,"outputs":[]}]}