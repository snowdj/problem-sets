{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DATA 1010\n",
        "\n",
        "#### 13 September 2019\n",
        "\n",
        "#### Matrix differentiation\n",
        "\n",
        "Just as elementary differentiation rules are helpful for optimizing single-variable functions, *matrix* differentiation rules are helpful for optimizing expressions written in matrix form. This technique is used often in statistics. \n",
        "\n",
        "Suppose $\\mathbf{f}$ is a function from $\\mathbb{R}^n$ to $\\mathbb{R}^m$. Writing $\\mathbf{f}(\\mathbf{x}) = \\mathbf{f}(x_1, \\ldots, x_n)$, we define the Jacobian matrix (or *derivative* matrix) to be \n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}} = \\left[\n",
        "  \\begin{array}{cccc}\n",
        "    \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n}  \\\\ \n",
        "    \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots & \\frac{\\partial f_2}{\\partial x_n}  \\\\ \n",
        "    \\vdots & & \\ddots & \\vdots \\\\ \n",
        "    \\frac{\\partial f_m}{\\partial x_1} & \\frac{\\partial f_m}{\\partial x_2} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n",
        "  \\end{array}\\right]\n",
        "$$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 1\n",
        "(a) The derivative with respect to $\\mathbf{x}$ goes by a more common name when $m = 1$. What is it?\n",
        "\n",
        "(b) In single-variable calculus, we learn that $f(x+h) - f(x) \\approx f'(x)h$ if $f$ is differentiable. Show that the vector derivative behaves similarly: if $f:\\mathbb{R}^n \\to \\mathbb{R}$, then $f(\\mathbf{x}+\\mathbf{h}) - f(\\mathbf{x}) \\approx \\frac{\\partial f}{\\partial \\mathbf{x}} \\mathbf{h}$. Check this result numerically below."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "f(x) = x[1]^2 + x[2]^2\n",
        "\n",
        "f([3.101,0.51]) - f([3.100,0.5])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 26,
          "data": {
            "text/plain": [
              "0.016300999999998567"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 26,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# replace verbal descriptions:\n",
        "#dfdx(x) = [first component of derivative, second component]'\n",
        "#dfdx([base point]) * [vector representing how far we moved from the base point]"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 2\n",
        "Show that if $A$ is a constant matrix, then \n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{\\partial}{\\partial \\mathbf{x}} (A \\mathbf{x}) &= A, \\text{ and}\\\\\n",
        "\\frac{\\partial}{\\partial \\mathbf{x}} (\\mathbf{x}' A) &= A'\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Hint: use the definition and work out the various partial derivatives with respect to $\\mathbf{x}$. If you feel stuck, make up a small example and try it."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 3\n",
        "\n",
        "Show that the Hessian of a function $f:\\mathbb{R}^n \\to \\mathbb{R}$ can be written as \n",
        "\n",
        "$$\n",
        "\\mathbf{H}(\\mathbf{x}) = \\frac{\\partial}{\\partial \\mathbf{x}}\n",
        "\\left(\\frac{\\partial f}{\\partial \\mathbf{x}}\\right)'.\n",
        "$$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 4\n",
        "\n",
        "Let $Q: \\mathbb{R}^n \\to \\mathbb{R}$ be defined by $Q(\\mathbf{x}) = \\mathbf{x}' A \\mathbf{x}$ where $Q$ is a symmetric matrix. Find $\\frac{\\partial Q}{\\partial \\mathbf{x}}.$ \n",
        "\n",
        "Note: the product rule for vector differentiation says that $\\frac{\\partial}{\\partial \\mathbf{x}} (\\mathbf{u}' \\mathbf{v}) = \\mathbf{u}'\\frac{\\partial \\mathbf{v}}{\\partial \\mathbf{x}} + \\mathbf{v}'\\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{x}}$ if $\\mathbf{u}$ and $\\mathbf{v}$ are vector-valued functions of $\\mathbf{x}$. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 5\n",
        "\n",
        "One way to think about Taylor polynomials is to observe that they **match derivatives** of a function at a given base point. For example, given a function $f$ from $\\mathbb{R}$ to $\\mathbb{R}$, \n",
        "\n",
        "$$\n",
        "f(0) + f'(0)x + \\frac{1}{2} f''(0) x^2\n",
        "$$\n",
        "\n",
        "is the unique quadratic polynomial with the property that its zeroth, first, and second derivatives match those of $f$. \n",
        "\n",
        "Use matrix differentiation to show that the same is true for functions of multiple variables with the quadratic function\n",
        "\n",
        "$$\n",
        "f(\\mathbf{0}) + \\frac{\\partial f}{\\partial \\mathbf{x}}(\\mathbf{0})\\mathbf{x} + \\frac{1}{2}\\mathbf{x}'H\\mathbf{x}, \n",
        "$$\n",
        "\n",
        "where $H$ is the Hessian of $f$ evaluated at the origin."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 6\n",
        "\n",
        "Suppose that $A$ is an $m\\times n$ matrix, that $\\mathbf{b} \\in \\mathbb{R}^m$, and that $\\lambda > 0$. Find a formula for the value of $\\mathbf{x}$ which minimizes: $|A\\mathbf{x} - \\mathbf{b}|^2 + \\lambda |\\mathbf{x}|^2$\n",
        "\n",
        "(For any square matrices that you would like to invert, you may assume they are invertible).\n",
        "\n",
        "Describe what happens (either to the minimizer, or to the original optimization problem) as $\\lambda$ increases from 0."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 7\n",
        "The result from the previous problem has a valuable data science interpretation. If we're trying to model the relationship between $A$ and $\\mathbf{b}$ as linear, then we're looking to find $\\mathbf{x}$ so as to minimize $|A\\mathbf{x} - \\mathbf{b}|^2$. \n",
        "\n",
        "However, if the columns of $A$ are close to being linearly dependent, then minimizing $|A\\mathbf{x} - \\mathbf{b}|^2$ often means leveraging directions that wouldn't be present if the columns of $A$ were adjusted slightly to make them actually linearly dependent. This gives a better fit than is meaningfully possible. \n",
        "\n",
        "(a) Using the matrix $A$ and vector $\\mathbf{b}$ given below, solve $A\\mathbf{x} = \\mathbf{b}$. Try adjusting the entry 1.1 to be even closer to 1. What happens to the coefficients? \n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Adding the term $\\lambda|\\mathbf{x}|^2$ tempers the coefficients and enables us to get a more meaningful result. In the plot below, show the minimizer of $|A\\mathbf{x} - \\mathbf{b}|^2 + \\lambda |\\mathbf{x}|^2$ for various values of $\\lambda$. Find one that you think is pretty good."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "using Plots\n",
        "gr(size=(400,400))\n",
        "A = [1.1 1\n",
        "      1  1]\n",
        "b = [0, 1]\n",
        "plot([(0,0),(A[1,1],A[2,1])], \n",
        "     label=\"first column of A\", \n",
        "     legend = :bottomright, \n",
        "     aspect_ratio = :equal)\n",
        "plot!([(0,0),(A[1,2],A[2,2])], label=\"second column of A\")\n",
        "plot!([(0,0),(b[1],b[2])], label=\"target (b)\")\n",
        "\n",
        "A \\ b"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 30,
          "data": {
            "text/plain": [
              "2-element Array{Float64,1}:\n",
              " -9.999999999999996\n",
              " 10.999999999999996"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 30,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 7 (challenge)\n",
        "\n",
        "We can also differentiate functions with respect to higher-order arrays like matrices. Just as the derivative of a real-valued function with respect to a vector is a vector, the derivative of a real-valued function with respect to a matrix is a matrix: given $f:\\mathbb{R}^{m\\times n} \\to \\mathbb{R}$, we define\n",
        "  $$\n",
        "    \\frac{\\partial}{\\partial W}f(W) =\n",
        "    \\begin{bmatrix}\n",
        "      \\frac{\\partial f}{\\partial a_{1,1}} & \\cdots & \\frac{\\partial\n",
        "        f}{\\partial a_{1,n}} \\\\\n",
        "      \\vdots & \\ddots & \\vdots \\\\\n",
        "      \\frac{\\partial f}{\\partial a_{m,1}} & \\cdots & \\frac{\\partial\n",
        "        f}{\\partial a_{m,n}}\n",
        "    \\end{bmatrix}, \n",
        "  $$\n",
        "  where $a_{i,j}$ is the entry in the $i$th row and $j$th column of\n",
        "  $W$. Suppose that $\\mathbf{u}$ is a $1 \\times m$ row vector and\n",
        "  $\\mathbf{v}$ is an $n \\times 1$ column vector. Show that\n",
        "  $$\n",
        "  \\frac{\\partial}{\\partial W}(\\mathbf{u}W \\mathbf{v}) = \\mathbf{u}'\n",
        "    \\mathbf{v}'. \n",
        "  $$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "julia-1.1"
    },
    "language_info": {
      "file_extension": ".jl",
      "name": "julia",
      "mimetype": "application/julia",
      "version": "1.1.1"
    },
    "kernelspec": {
      "name": "julia-1.1",
      "language": "julia",
      "display_name": "Julia 1.1.1"
    },
    "nteract": {
      "version": "0.14.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}